{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8e2b8c-b1e1-4dbe-939b-3f206e01d3f5",
   "metadata": {},
   "source": [
    "```\n",
    "!git clone https://github.com/amdegroot/ssd.pytorch.git\n",
    "!mv ssd300_mAP_77.43_v2.pth ssd.pytorch\n",
    "import os\n",
    "os.chdir('ssd.pytorch')\n",
    "!ls\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d4abcd-814a-4986-a027-246ada108a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-21 17:27:24--  https://s3.amazonaws.com/amdegroot-models/ssd300_mAP_77.43_v2.pth\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.4.30\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.4.30|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 105151288 (100M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘ssd300_mAP_77.43_v2.pth’\n",
      "\n",
      "100%[======================================>] 105,151,288 57.2MB/s   in 1.8s   \n",
      "\n",
      "2021-11-21 17:27:25 (57.2 MB/s) - ‘ssd300_mAP_77.43_v2.pth’ saved [105151288/105151288]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/amdegroot-models/ssd300_mAP_77.43_v2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b253b5-0fb9-493c-a5f9-6c9c98239409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Function, Variable\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97b2b60-9df4-42a1-bac6-f1a0ee7eaf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = {\n",
    "    '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n",
    "            512, 512, 512],\n",
    "    '512': [],\n",
    "}\n",
    "dataset_mean = (104, 117, 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed3f27b0-c600-44bf-8881-f09527fb93da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Norm(nn.Module):\n",
    "    def __init__(self,n_channels, scale):\n",
    "        super(L2Norm,self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.gamma = scale or None\n",
    "        self.eps = 1e-10\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.n_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.constant_(self.weight,self.gamma)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps\n",
    "        #x /= norm\n",
    "        x = torch.div(x,norm)\n",
    "        out = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a4f147-5e8c-45a7-8174-692a8eb60e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD(nn.Module):\n",
    "\n",
    "    def __init__(self, base):\n",
    "\n",
    "        super(SSD, self).__init__()\n",
    "\n",
    "        # SSD network\n",
    "        self.vgg = nn.ModuleList(base)\n",
    "\n",
    "        self.L2Norm = L2Norm(512, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # apply vgg up to conv4_3 relu\n",
    "        for k in range(23):\n",
    "            x = self.vgg[k](x)\n",
    "\n",
    "        s = self.L2Norm(x)\n",
    "\n",
    "        # apply vgg up to fc7\n",
    "        for k in range(23, len(self.vgg)):\n",
    "            x = self.vgg[k](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697ad169-0652-41e5-aeb6-a318dad596ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is derived from torchvision VGG make_layers()\n",
    "# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
    "def vgg(cfg, i, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = i\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        elif v == 'C':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "    layers += [pool5, conv6,\n",
    "               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f8e13b-c964-4d58-b7cd-677a9d20cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb07944f-b2b4-41de-9dac-92c694408524",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model = SSD(vgg(base['300'], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5abaccd8-8c61-40b2-af21-54b386ff2c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['extras.0.weight', 'extras.0.bias', 'extras.1.weight', 'extras.1.bias', 'extras.2.weight', 'extras.2.bias', 'extras.3.weight', 'extras.3.bias', 'extras.4.weight', 'extras.4.bias', 'extras.5.weight', 'extras.5.bias', 'extras.6.weight', 'extras.6.bias', 'extras.7.weight', 'extras.7.bias', 'loc.0.weight', 'loc.0.bias', 'loc.1.weight', 'loc.1.bias', 'loc.2.weight', 'loc.2.bias', 'loc.3.weight', 'loc.3.bias', 'loc.4.weight', 'loc.4.bias', 'loc.5.weight', 'loc.5.bias', 'conf.0.weight', 'conf.0.bias', 'conf.1.weight', 'conf.1.bias', 'conf.2.weight', 'conf.2.bias', 'conf.3.weight', 'conf.3.bias', 'conf.4.weight', 'conf.4.bias', 'conf.5.weight', 'conf.5.bias'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model.load_state_dict(torch.load('ssd300_mAP_77.43_v2.pth'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28cdac4f-32f4-462d-9b25-c350ab5cee59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSD(\n",
       "  (vgg): ModuleList(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "    (31): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (34): ReLU(inplace=True)\n",
       "  )\n",
       "  (L2Norm): L2Norm()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "915c1a88-8060-42cb-b2fc-941ec29292ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed\n",
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4100 images processed\n",
      "4200 images processed\n",
      "4300 images processed\n",
      "4400 images processed\n",
      "4500 images processed\n",
      "4600 images processed\n",
      "4700 images processed\n",
      "4800 images processed\n",
      "4900 images processed\n",
      "5000 images processed\n",
      "5100 images processed\n",
      "5200 images processed\n",
      "5300 images processed\n",
      "5400 images processed\n",
      "5500 images processed\n",
      "5600 images processed\n",
      "5700 images processed\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(), open('/mnt/data/VOCdevkit/VOC2012/ImageSets/Main/train.txt', 'r') as fp:\n",
    "\n",
    "    for i, line in enumerate(fp):\n",
    "\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "\n",
    "            continue\n",
    "        img = cv2.imread('/mnt/data/VOCdevkit/VOC2012/JPEGImages/' + line + '.jpg')\n",
    "        img = cv2.resize(img, (300, 300)).astype(np.float32)\n",
    "        img -= dataset_mean\n",
    "        img = img.astype(np.float32)\n",
    "        # to rgb\n",
    "        img = img[:, :, (2, 1, 0)]\n",
    "        x = torch.as_tensor(img, device=device).permute(2, 0, 1)\n",
    "        y = the_model(torch.unsqueeze(x, axis=0))\n",
    "        np.save('./data/coco_voc/train2012/' + line + '.npy', y.detach().cpu().numpy())\n",
    "\n",
    "        if i % 100 == 0:\n",
    "\n",
    "            print(f'{i} images processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60635913-fa4e-4fe0-88ae-9680eaf02c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed\n",
      "500 images processed\n",
      "1000 images processed\n",
      "1500 images processed\n",
      "2000 images processed\n",
      "2500 images processed\n",
      "3000 images processed\n",
      "3500 images processed\n",
      "4000 images processed\n",
      "4500 images processed\n",
      "5000 images processed\n",
      "5500 images processed\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(), open('/mnt/data/VOCdevkit/VOC2012/ImageSets/Main/val.txt', 'r') as fp:\n",
    "\n",
    "    for i, line in enumerate(fp):\n",
    "\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "\n",
    "            continue\n",
    "        img = cv2.imread('/mnt/data/VOCdevkit/VOC2012/JPEGImages/' + line + '.jpg')\n",
    "        img = cv2.resize(img, (300, 300)).astype(np.float32)\n",
    "        img -= dataset_mean\n",
    "        img = img.astype(np.float32)\n",
    "        # to rgb\n",
    "        img = img[:, :, (2, 1, 0)]\n",
    "        x = torch.as_tensor(img, device=device).permute(2, 0, 1)\n",
    "        y = the_model(torch.unsqueeze(x, axis=0))\n",
    "        np.save('./data/coco_voc/val2012/' + line + '.npy', y.detach().cpu().numpy())\n",
    "\n",
    "        if i % 500 == 0:\n",
    "\n",
    "            print(f'{i} images processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e462217-0004-4b56-957f-ea2029587be7",
   "metadata": {},
   "source": [
    "## Testing stuff below\n",
    "** Stop running here **\n",
    "(Can remove in later iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b59b46f1-917f-4e31-aad9-a513ec4d8340",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('/mnt/data/VOCdevkit/VOC2012/JPEGImages/2012_002961.jpg')\n",
    "img = cv2.resize(img, (300, 300)).astype(np.float32)\n",
    "img -= dataset_mean\n",
    "img = img.astype(np.float32)\n",
    "# to rgb\n",
    "img = img[:, :, (2, 1, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "551f2b38-a0f1-4cde-8404-059ea0da511a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "986f6283-925d-45d8-ab8d-43e9088f7fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(img).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3641945f-919a-4b26-a3f0-2e2e57015dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = the_model(torch.unsqueeze(x, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fa00a5a-9fe3-4e77-81d5-8c02335d8b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 19, 19])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3271839-0931-49f8-a3cd-a4038824e7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
